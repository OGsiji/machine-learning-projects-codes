{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reading in the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('samplesubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Date']=pd.to_datetime(train['Date'],format='%Y-%m-%d')\n",
    "test['Date']=pd.to_datetime(test['Date'],format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "dataframe=DataFrame()\n",
    "\n",
    "train['Year']=[train['Date'][i].year for i in range(len(train))]\n",
    "train['month']=[train['Date'][i].month for i in range(len(train))]\n",
    "train['day']=[train['Date'][i].day for i in range(len(train))]\n",
    "train.head()\n",
    "\n",
    "\n",
    "test['Year']=[test['Date'][i].year for i in range(len(test))]\n",
    "test['month']=[test['Date'][i].month for i in range(len(test))]\n",
    "test['day']=[test['Date'][i].day for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['min']=train['target_min']\n",
    "train['max']=train['target_max']\n",
    "train['variance']=train['target_variance']\n",
    "train['count']=train['target_count']\n",
    "\n",
    "test['min']=train['min']\n",
    "test['max']=train['max']\n",
    "test['variance']=train['variance']\n",
    "test['count']=train['count']\n",
    "\n",
    "train.drop(['target_min','target_variance','target_max','target_count'],inplace=True, axis=1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Place_ID','Date','Place_ID X Date'],inplace=True,axis=1)\n",
    "test.drop(['Place_ID','Date','Place_ID X Date'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dropping missing columns***\n",
    "\n",
    "After checking for the number of null values per columns using ***train.isnull().sum()*** I realized there are some columns with very high missing values e.g greater than 16000 out of 25000 records.\n",
    "so i decided to drop such columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.DataFrame(train.isnull().sum(), columns=['Na_sum'])\n",
    "tr.reset_index(inplace=True)\n",
    "\n",
    "tr_col = tr[tr['Na_sum']>15000]['index']\n",
    "\n",
    "tr_col = tr_col.to_list()\n",
    "\n",
    "train = train.drop(tr_col,axis=1)\n",
    "test = test.drop(tr_col,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures()\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures()\n",
    "to_cross = ['min', 'max', 'variance', 'count']\n",
    "crossed_feats = poly.fit_transform(train[to_cross].values)\n",
    "crossed_feats = poly.fit_transform(test[to_cross].values)\n",
    "\n",
    "#Convert to Pandas DataFrame and merge to original dataset\n",
    "crossed_feats = pd.DataFrame(crossed_feats)\n",
    "train = pd.concat([train, crossed_feats], axis=1)\n",
    "test = pd.concat([test, crossed_feats], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Treating Missing Values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.fillna(method = 'bfill', axis=1).fillna(0)\n",
    "test = test.fillna(method = 'bfill', axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***One Hot Encoding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['CTR_CATEGO_X'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did one hot encoding and drop the encoded column for category N since it doesn't appear in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.get_dummies(train, columns=['CTR_CATEGO_X'])\n",
    "\n",
    "#test = pd.get_dummies(test, columns=['CTR_CATEGO_X'])\n",
    "\n",
    "#train = train.drop(columns=['CTR_CATEGO_X_N'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Frequency Encoding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train['id'] = train['id'].map(train['id'].value_counts())\n",
    "\n",
    "#test['id'] = test['id'].map(test['id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***preparing the dataset for training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns='target')\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***More importation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost,lightgbm,catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.05, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainss,X_testss,y_trainss,y_testss = train_test_split(X,y, test_size = 0.4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "lgb = LGBMRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "cat = CatBoostRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Modelling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb.fit(X_train,y_train)\n",
    "xgb.fit(X_train,y_train,eval_metric = 'rmse', eval_set = [(X_train,y_train),\n",
    "                                                          (X_test,y_test)],\n",
    "            early_stopping_rounds = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb.fit(X_train,y_train)\n",
    "lgb.fit(X_train,y_train,eval_metric = 'rmse',\n",
    "            eval_set = [(X_train,y_train),(X_test,y_test)],\n",
    "            early_stopping_rounds = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat.fit(X_train,y_train)\n",
    "cat.fit(X_train,y_train, eval_set=[(X_train,y_train),\n",
    "                                   (X_test,y_test)], \n",
    "         early_stopping_rounds= 500, use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_trainss,y_trainss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbpred = xgb.predict(X_test)\n",
    "lgbpred = lgb.predict(X_test)\n",
    "rfpred = rf.predict(X_test)\n",
    "catpred = cat.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbpred_train = xgb.predict(X)\n",
    "lgbpred_train = lgb.predict(X)\n",
    "rfpred_train = rf.predict(X)\n",
    "catpred_train = cat.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGB >>>',rmse(y_test,xgbpred))\n",
    "print('LGB >>>',rmse(y_test,lgbpred))\n",
    "print('CAT >>>',rmse(y_test,catpred))\n",
    "print('RF >>>',rmse(y_test,rfpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGB >>>',rmse(y,xgbpred_train))\n",
    "print('LGB >>>',rmse(y,lgbpred_train))\n",
    "print('CAT >>>',rmse(y,catpred_train))\n",
    "print('RF >>>',rmse(y,rfpred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi = pd.Series(index=features, data= lgb.feature_importances_)\n",
    "_ = plt.figure(figsize=(10, 50))\n",
    "_ = fi.sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Columns that doesn't affect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame(index=features, data=lgb.feature_importances_)\n",
    "\n",
    "fi_df = fi_df.reset_index()\n",
    "fi_df.columns = ['cols','imp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df.sort_values(by = ['imp'],ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_useless = fi_df[fi_df['imp'] < 13]['cols'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(semi_useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(semi_useless,axis=1)\n",
    "test = test.drop(semi_useless,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns='target')\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use standard scaler\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#sc = StandardScaler()\n",
    "#train = sc.fit_transform(train)\n",
    "#test = sc.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)\n",
    "                                            \n",
    "    \n",
    "    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy1=lm.score(X_train, y_train)\n",
    "\n",
    "\n",
    "accuracy=lm.score(X_test, y_test)\n",
    "\n",
    "print(accuracy1,accuracy)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMRegressor(num_leaves=200, min_data_in_leaf=3,\n",
    "                    objective='regression',\n",
    "                    max_depth=-1,learning_rate=0.05, \n",
    "                    boosting_type='gbdt', \n",
    "                    feature_fraction=0.60,\n",
    "                    lambda_l1=1,lambda_l2=1, \n",
    "                    metric='rmse', \n",
    "                    num_iterations=4000)\n",
    "\n",
    "xgb = XGBRegressor(n_estimators = 5000, max_depth = 30, \n",
    "                     reg_lambda = 80,random_state = 30,\n",
    "                     learning_rate=0.3, gamma = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Lightgbm CrossVal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 23\n",
    "kf = KFold(n_splits=max_iter,shuffle=False,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_scores = []\n",
    "lgb_test_pred = np.zeros(len(test))\n",
    "lgb_train_pred = np.zeros(len(train))\n",
    "\n",
    "for fold,(tr_in,te_in) in enumerate(kf.split(X)):\n",
    "    \n",
    "    print(f\"==================================Fold{fold}=============================================\")\n",
    "    X_train,X_test = X.iloc[tr_in],X.iloc[te_in]\n",
    "    y_train,y_test = y.iloc[tr_in],y.iloc[te_in]\n",
    "    \n",
    "    lgb.fit(X_train,y_train,eval_metric = 'rmse', eval_set = [(X_train,y_train),(X_test,y_test)],\n",
    "            early_stopping_rounds = 500)\n",
    "    \n",
    "    lgb_scores.append(rmse(y_test,lgb.predict(X_test)))\n",
    "    \n",
    "    lgb_train_pred += lgb.predict(X)\n",
    "    lgb_test_pred += lgb.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy1=lgb.score(X_train, y_train)\n",
    "\n",
    "#accuracy2=lgb.score(X_test, y_test)\n",
    "\n",
    "#print(accuracy1,accuracy1=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lgb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = lgb_test_pred/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = final\n",
    "\n",
    "sub.to_csv('LgbSubmissionn.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = np.array(final, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = final\n",
    "\n",
    "sub.to_csv('LgbSubmissio.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC vs LB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***My best model was Lightgbm after I performed cross validation of 23 splits***\n",
    "\n",
    "Things you can vary to get better results\n",
    "\n",
    "* num_leaves = [100 - 200],\n",
    "* min_data_in_leaf = [3, 5, 10, 20, 30, 40],\n",
    "* learning_rate = [0.05,0.03,0.075], \n",
    "* feature_fraction = [0.60, 0.65, 0.50, 0.40, 0.35, 0.30],\n",
    "* num_iterations = [4000, 3500, 3000, 2500, 2200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Xgboost CrossVAl***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "kf = KFold(n_splits=max_iter,shuffle=False,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_scores = []\n",
    "xgb_test_pred = np.zeros(len(test))\n",
    "xgb_train_pred = np.zeros(len(train))\n",
    "\n",
    "for fold,(tr_in,te_in) in enumerate(kf.split(X)):\n",
    "    \n",
    "    print(f\"==================================Fold{fold}=============================================\")\n",
    "    X_train,X_test = X.iloc[tr_in],X.iloc[te_in]\n",
    "    y_train,y_test = y.iloc[tr_in],y.iloc[te_in]\n",
    "    \n",
    "    xgb.fit(X_train,y_train,eval_metric = 'rmse', eval_set = [(X_train,y_train),(X_test,y_test)],\n",
    "            early_stopping_rounds = 200)\n",
    "    \n",
    "    xgb_scores.append(rmse(y_test,xgb.predict(X_test)))\n",
    "    \n",
    "    xgb_train_pred += xgb.predict(X)\n",
    "    xgb_test_pred += xgb.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(xgb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = xgb_test_pred/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = final1\n",
    "\n",
    "sub.to_csv('XgbSubmissio.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
